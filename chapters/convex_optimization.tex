\chapter{Convex Optimization} \label{chap:convex_optimization}

In this chapter, we will introduce the two methods of convex optimization used in this thesis.
Both methods and varying parameters will be used to optimize NLSE and NWA.
The optimization problem solved by these methods is the unconstrained minimization problem
\[ \min_{\mathbf{x} \in \real^n} f(\mathbf{x}) \]
for a function \(f \colon \real^n \to \real\).
As discussed in \cref{sec:mathematical_description}, it suffices to solve an unconstrained minimization problem
in the case of minimizing weighted total wirelength.



\section{Gradient Descent} \label{sec:gradient_descent}

The idea of \emph{gradient descent} (GD) is simple:
Start with some vector \(\mathbf{x}^{(0)}\) and iterate the following process
\[ \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - h_k \nabla f \BktR{ \mathbf{x}^{(k)} } \]
until we are satisfied. 
The variable \(h_k\) is called the \emph{step size} and needs to be a positive number that may depend on the vectors of previous iterations.

Let us first inspect the theoretical convergence properties of this method:
\begin{theorem}[Corollary 2.1.2 in \cite{Nesterov-ConvexOptimization}] \label{thm:gradient_descent_converges}
 Let \(f: \real^n \to \real\) be a convex differentiable function with a Lipschitz continuous gradient:
 \[ \norm{ \nabla f(\mathbf{x}) - \nabla f(\mathbf{y}) }_2 \leq L \norm{\mathbf{x} - \mathbf{y}}_2 \quad \forall \ \mathbf{x}, \mathbf{y} \in \real^n\]
 Let the step size be \(h_k = 1/L\) for all \(k \in \integers\) 
 and \((\mathbf{x}^{(k)})_{k \in \integers}\) be the sequence generated by the gradient descent method.
 Then
 \[ f(\mathbf{x}^{(k)}) - f(\mathbf{x}^*) \leq \frac{2 L \norm{\mathbf{x}^{(0)} - \mathbf{x}^*}_2}{k + 4} \]
 where \(\mathbf{x}^*\) is any point at which \(f\) attains its minimum.
\end{theorem}

This theorem guarantees convergence to a point with a value that is close to optimal under three conditions:
The objective function is convex, differentiable and has a Lipschitz continuous gradient.
To check whether these conditions are fulfilled by our objective functions, will be discussed in the next chapter.

In this chapter, we want to discuss the parameter choices of the gradient descent method:
\begin{enumerate}
 \item What should \(\mathbf{x}^{(0)}\) be?
 \item How should we choose \(h_k\)?
 \item When do we stop the process?
\end{enumerate}



\subsection{The Start Vector} \label{subsec:start_vector}

The question of which start vector to use obviously depends on the use case
and thus cannot be answered in this general description of the gradient descent method.
The theorem above suggests that the start vector should be close to the solution vector.
Since very little is known about the solution vector in advance
one has to start with a best guess.
Note that even though a lower value of the objective function of a start vector can serve as an indicator
of the vector being close to the optimum
the guaranteed convergence in the above theorem only contains the distance
between the start vector and the closest optimal vector \(\norm{\mathbf{x}^{(0)} - \mathbf{x}^*}_2\)
and not the difference \(f(\mathbf{x}^{(0)}) - f(\mathbf{x}^*)\).



\subsection{The Step Size Strategy} \label{subsec:step_size_strategy}

The step size strategy is the most important aspect of the gradient descent method for its convergence properties.
If the step size is too small, gradient descent will converge extremely slowly, 
if it is too large the values may oscillate and can even diverge.
\Cref{thm:gradient_descent_converges} shows that a step size close to \(1/L^*\) (where \(L^*\) is the optimal Lipschitz constant of the gradient)
would yield the fastest method in the case in which the theorem proves convergence.
The best Lipschitz constant is often unknown and other step size strategies have been proposed.
In this thesis, we will consider the following:
\begin{itemize}
 \item \texttt{constant}: \(h_k = h_0\)
 \item \texttt{scaled-by-sqrt}: \(h_k = \frac{h_0}{\sqrt{k+1}}\)
 \item \texttt{backtracking-line-search}: \(h_k = 2^{-n} h_{k-1}\) with minimal \(n\) such that 
        \[f(\mathbf{x}^{(k)} - h_k \nabla f(\mathbf{x}^{(k)})) \leq f(\mathbf{x}^{(k)}) - \frac{1}{2} h_k \Norm{\nabla f(\mathbf{x}^{(k)})}_2^2 \]
 \item \texttt{observed-lipschitz}: \(h_k = \frac{\norm{ \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} }_2}{\norm{ \nabla f(\mathbf{x}^{(k)}) - \nabla f(\mathbf{x}^{(k-1)}) }_2}\)
 \item \texttt{min-observed-lipschitz}: \(h_k = \min\BktC{h_{k-1}, \frac{\norm{ \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} }_2}{\norm{ \nabla f(\mathbf{x}^{(k)}) - \nabla f(\mathbf{x}^{(k-1)}) }_2}}\)
 \item \texttt{barzilai-borwein}: \(h_k = \frac{\abs{ \BktR{ \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} }^T \BktR{ \nabla f(\mathbf{x}^{(k)}) - \nabla f(\mathbf{x}^{(k-1)}) } }}{\norm{ \nabla f(\mathbf{x}^{(k-1)}) - \nabla f(\mathbf{x}^{(k)}) }_2^2}\)
\end{itemize}

\texttt{constant} is the simplest method and by the theorem above it is also effective if a small Lipschitz constant is known.
But even if a Lipschitz constant can be derived by analytical means, it might be a suboptimal value with respect to the search space
and can lead to very slow convergence.

\texttt{scaled-by-sqrt} is a variant of the constant step size.
The step size is gradually lowered such that it will be low enough for convergence eventually.
Because it depends on a start step size \(h_0\), which has to be guessed, it has the same problems as a constant step size.
Both of the previous step size strategies are mentioned in \cite{Nesterov-ConvexOptimization}.

\texttt{backtracking-line-search} is a strategy that gradually lowers the step size
to guarantee that the objective function of the minimization sequence \(\mathbf{x}^{(k)}\) is decreasing enough.
If \(h_k \leq 1/L^*\), the condition is guaranteed to hold so the search process terminates and \(h_k \geq 1/(2L^*)\) for all \(k\).
The condition is called \emph{Armijo rule} and was proposed in \cite{Armijo-ArmijoRule}.
Armijo even showed that under slightly stronger requirements than in previous theorem convergence is guaranteed.

\texttt{observed-lipschitz} and \texttt{min-observed-lipschitz} estimate the reciprocal of the Lipschitz constant by the reciprocal of the 
\enquote{observed Lipschitz constant}.
While \texttt{observed-lipschitz} always uses the current observed Lipschitz constant, 
\texttt{min-observed-lipschitz} tries to get closer to the reciprocal of the actual Lipschitz constant by only ever decreasing the step size.
The \texttt{observed-lipschitz} method is used in \cite{LuChenChangShaHuangTengCheng-ePlace} as \enquote{Lipschitz constant prediction}
to avoid the additional runtime of \texttt{backtracking-line-search}.

Finally, \texttt{barzilai-borwein} is named after its inventors Barzilai and Borwein which proposed it in \cite{BarzilaiBorwein-StepSizeGradientMethods}.
Although convergence is only guaranteed for a more sophisticated adaptation of this strategy (see \cite{Raydan-GlobalBarzilaiBorwein}),
the good pratical results justify a direct use for our purposes.

Note that for all of theses step size strategies the very first step size \(h_0\) needs to be specified
because there are no previous values and previous gradients to calculate the step size.
This means that there are two independent parameters of the gradient descent method here:
The first step size \(h_0\) and the step size strategy.



\subsection{The Stopping Criterion} \label{subsec:stopping_criterion}

Because the gradient descent method is an iterative process, it has no obvious ending.
We have to specify a criterion that estimates if further iterations are likely to decrease the objective function by a considerable amount.
A standard choice is
\[ \norm{\nabla f(\mathbf{x}^{(k)})}_2 < \varepsilon \]
(see \cite[p. 466]{BoydVandenberghe-ConvexOptimization}).
A small norm of the gradient indicates that \(\mathbf{x}^{(k)}\) is close to a point \(\mathbf{x}\) satisfying \(\nabla f(\mathbf{x}) = 0\)
and if \(f\) is convex any such point is a global minimum.
In fact, if the objective function is strongly convex,
then a vector with a small norm of the gradient is provably close to optimum
depending on the strong convexity parameter.



\section{Nesterov's Accelerated Gradient} \label{sec:nesterovs_accelerated_gradient}

\emph{Nesterov's accelerated gradient} (NAG) method aims to improve the gradient descent method.
It was introduced by Yurii Nesterov in \cite{Nesterov-NesterovAcceleratedGradient}
and later reformulated as a momentum method by Sutskever et al. in the appendix of \cite{SutskeverMartensDahlHinten-MomentumInDeepLearning}.
We will use the easier momentum formulation.

The NAG method has two steps in each iteration:
\begin{align*}
 \widetilde{\mathbf{x}}^{(k+1)} &= \mathbf{x}^{(k)} + \mu_k \BktR{ \mathbf{x}^{(k)} - \mathbf{x}^{(k-1)} } \\
 \mathbf{x}^{(k+1)}             &= \widetilde{\mathbf{x}}^{(k+1)} - h_k \nabla f \BktR{ \widetilde{\mathbf{x}}^{(k+1)} }
\end{align*}
We will refer to the first as the \emph{momentum step} and to the second as the \emph{gradient step}.
The positive coefficient \(\mu_k\) is called the decay factor and Nesterov specifies that it should be \(\mu_k = (a_k - 1)/a_{k+1}\) where
\[ a_0 = 1 \quad \text{and} \quad a_{k+1} = \frac{1 + \sqrt{4 a_k^2 + 1}}{2}. \]
This way, \(\mu_k\) starts at \(0\) and tends to \(1\) as \(k\) increases.

The momentum step is added to go further in the direction that was determined in the last iteration.
Note that this is not the same as increasing the step size \(h_k\) because the direction \(\mathbf{x}^{(k)} - \mathbf{x}^{(k-1)}\)
is not only determined by the previous iteration's gradient step but also influenced by the momentum step of the previous iteration.

Again, we look at the theoretical convergence properties:
\begin{theorem}[Theorem 1 from \cite{Nesterov-NesterovAcceleratedGradient}] \label{thm:nesterovs_accelerated_gradient_converges}
 Let \(f: \real^n \to \real\) be a convex differentiable function with a Lipschitz continuous gradient:
 \[ \norm{ \nabla f(\mathbf{x}) - \nabla f(\mathbf{y}) }_2 \leq L \norm{\mathbf{x} - \mathbf{y}}_2 \quad \forall \ \mathbf{x}, \mathbf{y} \in \real^n\]
 Let the step size be \(h_k = 1/L\) for all \(k \in \integers\)
 and \((\mathbf{x}^{(k)})_{k \in \integers}\) be the sequence generated by the NAG method.
 Then
 \[ f(\mathbf{x}^{(k)}) - f(\mathbf{x}^*) \leq \frac{2 L \norm{\mathbf{x}^{(0)} - \mathbf{x}^*}_2}{(k + 2)^2} \]
 where \(\mathbf{x}^*\) is any point at which \(f\) attains its minimum.
\end{theorem}

The important part of this theorem is that the theoretical convergence of this method is faster than the convergence of the gradient descent method.
As for the gradient descent method, we must choose \(\mathbf{x}^{(0)}\), the step size strategy and the stopping criterion.
The step size strategy is the only aspect we need to consider again to address a subtle change.



\subsection{The Step Size Strategy} \label{subsec:NAG_step_size_strategy}

In the method of gradient descent there was only one sequence \((\mathbf{x}^{(k)})_{k \in \integers}\).
With the introduction of the momentum step there are two sequences 
\((\mathbf{x}^{(k)})_{k \in \integers}\) and \((\widetilde{\mathbf{x}}^{(k)})_{k \in \integers}\)
and we need to decide which of them shall be used for the calculation of the step size strategies mentioned in \cref{subsec:step_size_strategy}.
In this thesis, \((\widetilde{\mathbf{x}}^{(k)})_{k \in \integers}\)
will be used because hopefully, the computed step sizes are more relevant to the computed gradient (which also depends on \(\widetilde{\mathbf{x}}^{(k)}\))
and because this choice avoids the computation of the gradients \(\nabla f(\mathbf{x}^{(k)})\).
Also, in the special case of \texttt{backtracking-line-search} it allows us to use that Nesterov showed in \cite{Nesterov-NesterovAcceleratedGradient}
that choosing the step size as \(h_k = 2^{-n} h_{k-1}\) with minimal \(n\) such that 
\[f(\widetilde{\mathbf{x}}^{(k)} - h_k \nabla f(\widetilde{\mathbf{x}}^{(k)})) \leq f(\widetilde{\mathbf{x}}^{(k)}) - \frac{1}{2} h_k \Norm{\nabla f(\widetilde{\mathbf{x}}^{(k)})}_2^2 \]
leads to a similar guaranteed convergence as the one presented in \cref{thm:nesterovs_accelerated_gradient_converges}.
