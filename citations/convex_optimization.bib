@Inbook{Nesterov-ConvexOptimization,
  author="Nesterov, Yurii",
  title="Smooth Convex Optimization",
  bookTitle="Introductory Lectures on Convex Optimization: A Basic Course",
  year="2004",
  publisher="Springer US",
  address="Boston, MA",
  pages="51--110",
  isbn="978-1-4419-8853-9",
  doi="10.1007/978-1-4419-8853-9_2",
  url="https://doi.org/10.1007/978-1-4419-8853-9_2"
}

@article{Nesterov-NesterovAcceleratedGradient,
  author="Nesterov, Yurii",
  title="A method for solving the convex programming problem with convergence rate $O(1/k^2)$",
  journal="Dokl. Akad. Nauk SSSR",
  year="1983",
  volume="269",
  pages="543-547",
  URL="https://ci.nii.ac.jp/naid/10029946121/en/"
}

@article{Armijo-ArmijoRule,
  author = "Armijo, Larry",
  fjournal = "Pacific Journal of Mathematics",
  journal = "Pacific J. Math.",
  number = "1",
  pages = "1--3",
  publisher = "Pacific Journal of Mathematics, A Non-profit Corporation",
  title = "Minimization of functions having Lipschitz continuous first partial derivatives.",
  url = "https://projecteuclid.org:443/euclid.pjm/1102995080",
  volume = "16",
  year = "1966"
}

@inbook{BoydVandenberghe-ConvexOptimization, 
  place={Cambridge}, 
  title={Convex functions}, 
  DOI={10.1017/CBO9780511804441.004}, 
  booktitle={Convex Optimization}, 
  publisher={Cambridge University Press}, 
  author={Boyd, Stephen and Vandenberghe, Lieven}, 
  year={2004}, 
  pages={67–126}
}

@inproceedings{LangeZuehlkeHolzVillmann-SmoothMaximum,
  title={Applications of lp-Norms and their Smooth Approximations for Gradient Based Learning Vector Quantization},
  author={Mandy Lange and Dietlind Z{\"u}hlke and Olaf Holz and Thomas Villmann},
  booktitle={ESANN},
  year={2014}
}

@article{Cook-BasicPropertiesOfTheSoftMaximum,
  title={Basic properties of the soft maximum},
  author={Cook, John D},
  year={2011},
  publisher={bepress}
}

@article{GaoPavel-PropertiesOfTheSoftmaxFunction,
  title={On the properties of the softmax function with application in game theory and reinforcement learning},
  author={Gao, Bolin and Pavel, Lacra},
  journal={arXiv preprint arXiv:1704.00805},
  year={2017}
}

@article{DurkanPapamakariosMurray-SequentialNeuralMethods,
       author = {{Durkan}, Conor and {Papamakarios}, George and {Murray}, Iain},
        title = "{Sequential Neural Methods for Likelihood-free Inference}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2018,
        month = nov,
          eid = {arXiv:1811.08723},
        pages = {arXiv:1811.08723},
archivePrefix = {arXiv},
       eprint = {1811.08723},
 primaryClass = {stat.ML},
}

@book{GolubVanLoan-MatrixComputations,
  author = {Golub, Gene H. and van Loan, Charles F.},
  edition = {Fourth},
  publisher = {JHU Press},
  title = {Matrix Computations},
  url = {http://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm},
  year = 2013
}

@inbook{HornJohnson-PositiveDefiniteAndSemidefiniteMatrices, 
  place={Cambridge},
  edition={2},
  title={Positive Definite and Semidefinite Matrices},
  DOI={10.1017/9781139020411.010},
  booktitle={Matrix Analysis},
  publisher={Cambridge University Press},
  author={Horn, Roger A. and Johnson, Charles R.},
  year={2012},
  pages={425–516}
}

@article{BarzilaiBorwein-StepSizeGradientMethods,
  author = {Barzilai, Jonathan and Borwein, Jonathan M.},
  title = "{Two-Point Step Size Gradient Methods}",
  journal = {IMA Journal of Numerical Analysis},
  volume = {8},
  number = {1},
  pages = {141-148},
  year = {1988},
  month = {01},
  abstract = "{We derive two-point step sizes for the steepest-descent method by approximating the secant equation. At the cost of storage of an extra iterate and gradient, these algorithms achieve better performance and cheaper computation than the classical steepest-descent method. We indicate a convergence analysis of the method in the two-dimensional quadratic case. The behaviour is highly remarkable and the analysis entirely nonstandard.}",
  issn = {0272-4979},
  doi = {10.1093/imanum/8.1.141},
  url = {https://doi.org/10.1093/imanum/8.1.141},
  eprint = {https://academic.oup.com/imajna/article-pdf/8/1/141/2402762/8-1-141.pdf},
}

@article{Raydan-GlobalBarzilaiBorwein,
  author = {Raydan, Marcos},
  title = {The Barzilai and Borwein Gradient Method for the Large Scale Unconstrained Minimization Problem},
  journal = {SIAM Journal on Optimization},
  volume = {7},
  number = {1},
  pages = {26-33},
  year = {1997},
  doi = {10.1137/S1052623494266365},
  URL = {https://doi.org/10.1137/S1052623494266365},
  eprint = {https://doi.org/10.1137/S1052623494266365}
}

@InProceedings{SutskeverMartensDahlHinten-MomentumInDeepLearning,
  title = 	 {On the importance of initialization and momentum in deep learning},
  author = 	 {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1139--1147},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  month =	 {06},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/sutskever13.pdf},
  url = 	 {http://proceedings.mlr.press/v28/sutskever13.html},
  abstract = 	 {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   }
}
